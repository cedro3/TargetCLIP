{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TargetCLIP",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cedro3/TargetCLIP/blob/main/TargetCLIP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suZU750tUcAm"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n517s4aGUZfL"
      },
      "source": [
        "#@title e4e Setup (may take a few minutes)\n",
        "import os\n",
        "os.chdir('/content')\n",
        "CODE_DIR = 'encoder4editing'\n",
        "\n",
        "%tensorflow_version 1.x\n",
        "! pip install torch==1.7.1+cu110 torchvision==0.8.2+cu110 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html \n",
        "\n",
        "!git clone https://github.com/omertov/encoder4editing.git $CODE_DIR\n",
        "!wget https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip\n",
        "!sudo unzip ninja-linux.zip -d /usr/local/bin/\n",
        "!sudo update-alternatives --install /usr/bin/ninja ninja /usr/local/bin/ninja 1 --force\n",
        "os.chdir(f'./{CODE_DIR}')\n",
        "\n",
        "from argparse import Namespace\n",
        "import time\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "sys.path.append(\".\")\n",
        "sys.path.append(\"..\")\n",
        "\n",
        "from utils.common import tensor2im\n",
        "from models.psp import pSp  # we use the pSp framework to load the e4e encoder.\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "\n",
        "# --- Download e4e model ---\n",
        "experiment_type = 'ffhq_encode'\n",
        "def get_download_model_command(file_id, file_name):\n",
        "    \"\"\" Get wget download command for downloading the desired model and save to directory pretrained_models. \"\"\"\n",
        "    current_directory = os.getcwd()\n",
        "    save_path = os.path.join(os.path.dirname(current_directory), CODE_DIR, \"pretrained_models\")\n",
        "    if not os.path.exists(save_path):\n",
        "        os.makedirs(save_path)\n",
        "    url = r\"\"\"wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id={FILE_ID}' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id={FILE_ID}\" -O {SAVE_PATH}/{FILE_NAME} && rm -rf /tmp/cookies.txt\"\"\".format(FILE_ID=file_id, FILE_NAME=file_name, SAVE_PATH=save_path)\n",
        "    return url    \n",
        "\n",
        "MODEL_PATHS = {\n",
        "    \"ffhq_encode\": {\"id\": \"1cUv_reLE6k3604or78EranS7XzuVMWeO\", \"name\": \"e4e_ffhq_encode.pt\"},\n",
        "    \"cars_encode\": {\"id\": \"17faPqBce2m1AQeLCLHUVXaDfxMRU2QcV\", \"name\": \"e4e_cars_encode.pt\"},\n",
        "    \"horse_encode\": {\"id\": \"1TkLLnuX86B_BMo2ocYD0kX9kWh53rUVX\", \"name\": \"e4e_horse_encode.pt\"},\n",
        "    \"church_encode\": {\"id\": \"1-L0ZdnQLwtdy6-A_Ccgq5uNJGTqE7qBa\", \"name\": \"e4e_church_encode.pt\"}\n",
        "}\n",
        "\n",
        "path = MODEL_PATHS[experiment_type]\n",
        "download_command = get_download_model_command(file_id=path[\"id\"], file_name=path[\"name\"]) \n",
        "\n",
        "!wget {download_command}\n",
        "\n",
        "\n",
        "# --- e4e setup ---\n",
        "! pip install --upgrade gdown\n",
        "import gdown\n",
        "gdown.download('https://drive.google.com/u/0/uc?id=1jZnwdPOXhte2gseETvTwRtx8r8TT196J', '/content/encoder4editing/e4e_ffhq_encode.pt', quiet=False)\n",
        "experiment_type = 'ffhq_encode'\n",
        "\n",
        "os.chdir('/content/encoder4editing')\n",
        "\n",
        "EXPERIMENT_ARGS = {\n",
        "        \"model_path\": \"e4e_ffhq_encode.pt\"\n",
        "    }\n",
        "EXPERIMENT_ARGS['transform'] = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
        "resize_dims = (256, 256)\n",
        "\n",
        "model_path = EXPERIMENT_ARGS['model_path']\n",
        "ckpt = torch.load(model_path, map_location='cpu')\n",
        "opts = ckpt['opts']\n",
        "# pprint.pprint(opts)  # Display full options used\n",
        "# update the training options\n",
        "opts['checkpoint_path'] = model_path\n",
        "opts= Namespace(**opts)\n",
        "net = pSp(opts)\n",
        "net.eval()\n",
        "net.cuda()\n",
        "print('Model successfully loaded!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlGH0NmOVEde"
      },
      "source": [
        "#@title TargetCLIP Setup\n",
        "import os\n",
        "!git clone https://github.com/cedro3/TargetCLIP.git\n",
        "os.chdir(f'./TargetCLIP')\n",
        "\n",
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# downloads StyleGAN's weights\n",
        "ids = ['1EM87UquaoQmk17Q8d5kYIAHqu0dkYqdT']\n",
        "for file_id in ids:\n",
        "  downloaded = drive.CreateFile({'id':file_id})\n",
        "  downloaded.FetchMetadata(fetch_all=True)\n",
        "  downloaded.GetContentFile(downloaded.metadata['title'])\n",
        "\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "os.chdir(f'../TargetCLIP')\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "from torch import optim\n",
        "from tqdm import tqdm\n",
        "import clip\n",
        "\n",
        "# from criteria.clip_loss import CLIPLoss\n",
        "from models.stylegan2.model import Generator\n",
        "import math\n",
        "import copy\n",
        "\n",
        "\n",
        "# aux function\n",
        "def get_latent(args, g_ema):\n",
        "    mean_latent = g_ema.mean_latent(4096)\n",
        "    latent_code_init_not_trunc = torch.randn(1, 512).cuda()\n",
        "    with torch.no_grad():\n",
        "        # _, latent_code_init = g_ema([latent_code_init_not_trunc], return_latents=True,\n",
        "        #                             truncation=args.truncation, truncation_latent=mean_latent)\n",
        "        _, latent_code_init,_ = g_ema([latent_code_init_not_trunc], return_latents=True,\n",
        "                                    truncation=args.truncation, truncation_latent=mean_latent)\n",
        "\n",
        "    direction = latent_code_init.detach().clone()\n",
        "    direction.requires_grad = True\n",
        "    return direction\n",
        "\n",
        "\n",
        "def load_model(args):\n",
        "    g_ema = Generator(args.stylegan_size, 512, 8)\n",
        "    g_ema.load_state_dict(torch.load(args.ckpt)[\"g_ema\"], strict=False)\n",
        "    g_ema.eval()\n",
        "    g_ema = g_ema.cuda()\n",
        "    return g_ema\n",
        "\n",
        "\n",
        "def get_lr(t, initial_lr, rampdown=0.75, rampup=0.005):\n",
        "    lr_ramp = min(1, (1 - t) / rampdown)\n",
        "    lr_ramp = 0.5 - 0.5 * math.cos(lr_ramp * math.pi)\n",
        "    lr_ramp = lr_ramp * min(1, t / rampup)\n",
        "\n",
        "    return initial_lr * lr_ramp\n",
        "\n",
        "args = {\n",
        "    \"ckpt\": \"stylegan2-ffhq-config-f.pt\",\n",
        "    \"stylegan_size\": 1024,\n",
        "    \"lr\": 0.1,\n",
        "    \"truncation\": 0.7,\n",
        "    \"save_intermediate_image_every\": 1,\n",
        "    \"results_dir\": \"results\",\n",
        "    \"dir_name\": \"results\",\n",
        "    \"num_batches\": 1,\n",
        "    \"real_images\": True,\n",
        "    \"data_path\": \"train_faces.pt\",\n",
        "}\n",
        "\n",
        "from argparse import Namespace\n",
        "a=Namespace(**args)\n",
        "\n",
        "g_ema = load_model(a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjGbAy2ov4zD"
      },
      "source": [
        "## Manipulate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuScCzNyUtSS"
      },
      "source": [
        "#@markdown Upload an image to the encoder4editing folder and set the image_name into the image name\n",
        "image_name = './TargetCLIP/pic/01.png' #@param {type:\"string\"}\n",
        "os.chdir('/content/encoder4editing')\n",
        "EXPERIMENT_DATA_ARGS = {\n",
        "    \"ffhq_encode\": {\n",
        "        \"model_path\": \"pretrained_models/e4e_ffhq_encode.pt\",\n",
        "        \"image_path\": image_name\n",
        "    }\n",
        "    \n",
        "}\n",
        "# Setup required image transformations\n",
        "EXPERIMENT_ARGS = EXPERIMENT_DATA_ARGS[experiment_type]\n",
        "if experiment_type == 'cars_encode':\n",
        "    EXPERIMENT_ARGS['transform'] = transforms.Compose([\n",
        "            transforms.Resize((192, 256)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
        "    resize_dims = (256, 192)\n",
        "else:\n",
        "    EXPERIMENT_ARGS['transform'] = transforms.Compose([\n",
        "        transforms.Resize((256, 256)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
        "    resize_dims = (256, 256)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDOP3WWfUydG"
      },
      "source": [
        "#@title Show aligned original image\n",
        "image_path = EXPERIMENT_DATA_ARGS[experiment_type][\"image_path\"]\n",
        "original_image = Image.open(image_path)\n",
        "original_image = original_image.convert(\"RGB\")\n",
        "if experiment_type == \"ffhq_encode\" and 'shape_predictor_68_face_landmarks.dat' not in os.listdir():\n",
        "    !wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
        "    !bzip2 -dk shape_predictor_68_face_landmarks.dat.bz2\n",
        "\n",
        "def run_alignment(image_path):\n",
        "  import dlib\n",
        "  from utils.alignment import align_face\n",
        "  predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
        "  aligned_image = align_face(filepath=image_path, predictor=predictor) \n",
        "  print(\"Aligned image has shape: {}\".format(aligned_image.size))\n",
        "  return aligned_image \n",
        "\n",
        "if experiment_type == \"ffhq_encode\":\n",
        "  input_image = run_alignment(image_path)\n",
        "else:\n",
        "  input_image = original_image\n",
        "\n",
        "input_image.resize(resize_dims)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1-9zkFoU0Ua"
      },
      "source": [
        "#@title Invert\n",
        "img_transforms = EXPERIMENT_ARGS['transform']\n",
        "transformed_image = img_transforms(input_image)\n",
        "\n",
        "def display_alongside_source_image(result_image, source_image):\n",
        "    res = np.concatenate([np.array(source_image.resize(resize_dims)),\n",
        "                          np.array(result_image.resize(resize_dims))], axis=1)\n",
        "    return Image.fromarray(res)\n",
        "\n",
        "def run_on_batch(inputs, net):\n",
        "    images, latents = net(inputs.to(\"cuda\").float(), randomize_noise=False, return_latents=True)\n",
        "    if experiment_type == 'cars_encode':\n",
        "        images = images[:, :, 32:224, :]\n",
        "    return images, latents\n",
        "\n",
        "with torch.no_grad():\n",
        "    tic = time.time()\n",
        "    images, latents = run_on_batch(transformed_image.unsqueeze(0), net)\n",
        "    result_image, latent = images[0], latents[0]\n",
        "    toc = time.time()\n",
        "    print('Inference took {:.4f} seconds.'.format(toc - tic))\n",
        "\n",
        "# Display inversion:\n",
        "display_alongside_source_image(tensor2im(result_image), input_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwR05NO9VOuM"
      },
      "source": [
        "#@title Chose target\n",
        "os.chdir('/content/encoder4editing/TargetCLIP')\n",
        "\n",
        "dirs = {\n",
        "  'Elsa': 'dirs/elsa.npy',\n",
        "  'Pocahontas': 'dirs/pocahontas.npy',\n",
        "  'Keanu Reeves': 'dirs/keanu.npy',\n",
        "  'Trump': 'dirs/trump.npy',\n",
        "  'Joker': 'dirs/joker.npy',\n",
        "  'Ariel': 'dirs/ariel.npy',\n",
        "  'Doc Brown': 'dirs/doc.npy',\n",
        "  'Beyonce': 'dirs/beyonce.npy',\n",
        "  'Morgan Freeman': 'dirs/morgan.npy',\n",
        "}\n",
        "\n",
        "targets = {\n",
        "  'Elsa': 'dirs/targets/elsa.jpg',   \n",
        "  'Pocahontas': 'dirs/targets/pocahontas.jpg',\n",
        "  'Keanu Reeves': 'dirs/targets/keanu.jpg',\n",
        "  'Trump': 'dirs/targets/trump.jpg',\n",
        "  'Joker': 'dirs/targets/joker.jpg',\n",
        "  'Ariel': 'dirs/targets/ariel.jpeg',\n",
        "  'Doc Brown': 'dirs/targets/doc_brown.jpg',\n",
        "  'Beyonce': 'dirs/targets/beyonce.jpg',\n",
        "  'Morgan Freeman': 'dirs/targets/morgan_freeman.jpg',\n",
        "}\n",
        "\n",
        "sources_ids = {\n",
        "  'Taylor Swift': 67,   \n",
        "  'Elon Musk': 4,\n",
        "  'Hillary Clinton': 9,\n",
        "  'Alfie Allen': 34,\n",
        "  'Obama': 61\n",
        "}\n",
        "\n",
        "target = 'Elsa' #@param ['Trump','Keanu Reeves', 'Elsa', 'Pocahontas', 'Joker', 'Ariel', 'Doc Brown', 'Beyonce', 'Morgan Freeman']\n",
        "source = latents\n",
        "dir = torch.from_numpy(np.load(dirs[target]))\n",
        "#dir = torch.load('dirs/10me.pt')\n",
        "target_path = targets[target]\n",
        "\n",
        "\n",
        "# title Show target image\n",
        "assert(target_path is not None)\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "img = mpimg.imread(target_path)\n",
        "imgplot = plt.imshow(img)\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vtp4IM0VWZP"
      },
      "source": [
        "#@title Show manipulation on source\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "#alpha=1\n",
        "alpha = 1.4 #@param {type:\"slider\", min:0, max:2, step:0.1}\n",
        "dir = dir.cuda()\n",
        "source = source.cuda()\n",
        "source_img, _ = g_ema([source], input_is_latent=True, randomize_noise=False)\n",
        "source_amp, _ = g_ema([source + dir * alpha], input_is_latent=True,\n",
        "                        randomize_noise=False)\n",
        "\n",
        "torchvision.utils.save_image(source_img, f\"results_orig.png\", normalize=True, range=(-1, 1))\n",
        "torchvision.utils.save_image(source_amp, f\"results_manipulated.png\", normalize=True, range=(-1, 1))\n",
        "plt.figure(figsize=(14,7), dpi= 100)\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(mpimg.imread('results_orig.png'))\n",
        "plt.title('original')\n",
        "plt.axis('off')\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(mpimg.imread('results_manipulated.png'))\n",
        "plt.title('manipulated')\n",
        "plt.axis('off')\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_yIgDqzWedA"
      },
      "source": [
        "#@title Make manipulated movie\n",
        "import os \n",
        "import shutil\n",
        "from tqdm import trange\n",
        "\n",
        "# reset images folder\n",
        "if os.path.isdir('images'):\n",
        "    shutil.rmtree('images')\n",
        "os.makedirs('images', exist_ok=True)\n",
        "\n",
        "# delete output.mp4 \n",
        "if os.path.exists('./output.mp4'):\n",
        "   os.remove('./output.mp4')\n",
        "\n",
        "alpha = 1.4 #@param {type:\"slider\", min:0, max:2, step:0.1}\n",
        "dir = dir.cuda()\n",
        "source = source.cuda()\n",
        "\n",
        "end = int(alpha * 100)\n",
        "for i in trange(0, end):\n",
        "  source_amp, _ = g_ema([source + dir * i/100], input_is_latent=True,\n",
        "                        randomize_noise=False)\n",
        "  torchvision.utils.save_image(source_amp, 'images/'+str(i).zfill(4)+'.png', normalize=True, range=(-1, 1))\n",
        "\n",
        "# make movie from png in images folder\n",
        "! ffmpeg -r 30 -i images/%4d.png\\\n",
        "               -vcodec libx264 -pix_fmt yuv420p output.mp4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYSmH4PObLk6"
      },
      "source": [
        "#@title play movie\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "mp4 = open('./output.mp4', 'rb').read()\n",
        "data_url = 'data:video/mp4;base64,' + b64encode(mp4).decode()\n",
        "HTML(f\"\"\"\n",
        "<video width=\"70%\" height=\"70%\" controls>\n",
        "      <source src=\"{data_url}\" type=\"video/mp4\">\n",
        "</video>\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
